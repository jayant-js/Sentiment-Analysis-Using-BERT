{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12437668,"sourceType":"datasetVersion","datasetId":7845519}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:20:10.298280Z","iopub.execute_input":"2025-08-30T12:20:10.298534Z","iopub.status.idle":"2025-08-30T12:20:13.437373Z","shell.execute_reply.started":"2025-08-30T12:20:10.298515Z","shell.execute_reply":"2025-08-30T12:20:13.436462Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/IMDB Dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.auto import tqdm\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:20:17.182429Z","iopub.execute_input":"2025-08-30T12:20:17.182736Z","iopub.status.idle":"2025-08-30T12:20:57.713973Z","shell.execute_reply.started":"2025-08-30T12:20:17.182714Z","shell.execute_reply":"2025-08-30T12:20:57.713343Z"}},"outputs":[{"name":"stderr","text":"2025-08-30 12:20:40.258731: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756556440.628018      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756556440.738518      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/IMDB Dataset.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:20:57.715341Z","iopub.execute_input":"2025-08-30T12:20:57.715956Z","iopub.status.idle":"2025-08-30T12:20:59.267210Z","shell.execute_reply.started":"2025-08-30T12:20:57.715937Z","shell.execute_reply":"2025-08-30T12:20:59.266324Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"df = df.drop_duplicates().reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:20:59.268111Z","iopub.execute_input":"2025-08-30T12:20:59.268411Z","iopub.status.idle":"2025-08-30T12:20:59.475281Z","shell.execute_reply.started":"2025-08-30T12:20:59.268381Z","shell.execute_reply":"2025-08-30T12:20:59.474551Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def preprocessing(text:str) -> str:\n    # def lower_casing(text:str) -> str:\n    #     return text.lower()\n\n    def remove_html_tags(text:str) -> str:\n        return re.sub(r\"<[^>]+>\", \"\", text)\n\n    def remove_url(text:str) -> str:\n        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n    \n    # def remove_punctuation(text:str) -> str:\n    #     return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n    \n    # STOP_WORDS = set(stopwords.words('english'))\n    # def remove_stop_words(text:str) -> str:\n    #     return \" \".join(word for word in text.split() if word not in STOP_WORDS)\n    \n    # text = lower_casing(text)\n    text = remove_html_tags(text)\n    text = remove_url(text)\n    # text = remove_punctuation(text)\n    # text = remove_stop_words(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:20:59.477272Z","iopub.execute_input":"2025-08-30T12:20:59.477510Z","iopub.status.idle":"2025-08-30T12:20:59.482638Z","shell.execute_reply.started":"2025-08-30T12:20:59.477491Z","shell.execute_reply":"2025-08-30T12:20:59.481803Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df['review'] = df['review'].apply(preprocessing)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:20:59.483512Z","iopub.execute_input":"2025-08-30T12:20:59.483785Z","iopub.status.idle":"2025-08-30T12:21:00.452008Z","shell.execute_reply.started":"2025-08-30T12:20:59.483757Z","shell.execute_reply":"2025-08-30T12:21:00.451467Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"le = LabelEncoder()\ndf['label'] = le.fit_transform(df['sentiment'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:21:00.452827Z","iopub.execute_input":"2025-08-30T12:21:00.453112Z","iopub.status.idle":"2025-08-30T12:21:00.468088Z","shell.execute_reply.started":"2025-08-30T12:21:00.453086Z","shell.execute_reply":"2025-08-30T12:21:00.467130Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"le.classes_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:21:00.469652Z","iopub.execute_input":"2025-08-30T12:21:00.470152Z","iopub.status.idle":"2025-08-30T12:21:00.494961Z","shell.execute_reply.started":"2025-08-30T12:21:00.470119Z","shell.execute_reply":"2025-08-30T12:21:00.494196Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array(['negative', 'positive'], dtype=object)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:21:00.495733Z","iopub.execute_input":"2025-08-30T12:21:00.496009Z","iopub.status.idle":"2025-08-30T12:21:01.951533Z","shell.execute_reply.started":"2025-08-30T12:21:00.495990Z","shell.execute_reply":"2025-08-30T12:21:01.950814Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c0defdaa4724be490d77087068437a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c13ae2f49f1e4eea931a3b60bb5f4652"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca832e57eb594b74a7bf878c7709fd3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc875d1dec664ea7ad4a99960d322272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f28f04bb0aec43109e50a6435cbcc1e5"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"encodings = tokenizer(\n    df['review'].to_list(),\n    truncation=True,\n    padding=True,\n    max_length=512,\n    return_tensors='pt'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:21:01.952341Z","iopub.execute_input":"2025-08-30T12:21:01.952645Z","iopub.status.idle":"2025-08-30T12:22:52.439925Z","shell.execute_reply.started":"2025-08-30T12:21:01.952621Z","shell.execute_reply":"2025-08-30T12:22:52.439042Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:22:52.442269Z","iopub.execute_input":"2025-08-30T12:22:52.442838Z","iopub.status.idle":"2025-08-30T12:22:52.446730Z","shell.execute_reply.started":"2025-08-30T12:22:52.442816Z","shell.execute_reply":"2025-08-30T12:22:52.446068Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class SentimentDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:22:52.447600Z","iopub.execute_input":"2025-08-30T12:22:52.447809Z","iopub.status.idle":"2025-08-30T12:22:52.468027Z","shell.execute_reply.started":"2025-08-30T12:22:52.447783Z","shell.execute_reply":"2025-08-30T12:22:52.467084Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_texts, val_texts, train_labels, val_labels, train_indices, val_indices = train_test_split(\n    df['review'], df['label'], df.index, test_size=0.2, random_state=42\n)\n\ntrain_encodings = {key: val[train_indices] for key, val in encodings.items()}\nval_encodings = {key: val[val_indices] for key, val in encodings.items()}\n\ntrain_dataset = SentimentDataset(train_encodings, train_labels.to_list())\nval_dataset = SentimentDataset(val_encodings, val_labels.to_list())\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32)\nprint(\"PyTorch DataLoaders created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:22:52.469523Z","iopub.execute_input":"2025-08-30T12:22:52.469824Z","iopub.status.idle":"2025-08-30T12:22:52.807069Z","shell.execute_reply.started":"2025-08-30T12:22:52.469798Z","shell.execute_reply":"2025-08-30T12:22:52.806237Z"}},"outputs":[{"name":"stdout","text":"PyTorch DataLoaders created.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained(\n    'roberta-base',\n    num_labels=len(df['label'].unique())\n)\nmodel.to(device)\nprint(\"Model loaded and moved to device.\")\n\nif torch.cuda.device_count() > 1:\n  print(f\"Using {torch.cuda.device_count()} GPUs for Data Parallel training.\")\n  model = torch.nn.DataParallel(model)\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\nnum_training_steps = len(train_dataloader) * 3\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\nprint(\"Optimizer and scheduler configured.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:22:52.807983Z","iopub.execute_input":"2025-08-30T12:22:52.808269Z","iopub.status.idle":"2025-08-30T12:22:56.176367Z","shell.execute_reply.started":"2025-08-30T12:22:52.808250Z","shell.execute_reply":"2025-08-30T12:22:56.175542Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f336f9c48e40d4979bf1fbc1e26b83"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded and moved to device.\nUsing 2 GPUs for Data Parallel training.\nOptimizer and scheduler configured.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"for epoch in range(3):\n    print(f\"\\n--- Epoch {epoch + 1}/{3} ---\")\n    \n    model.train()\n    total_train_loss = 0\n    progress_bar = tqdm(train_dataloader, desc=\"Training\")\n    for batch in progress_bar:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        optimizer.zero_grad()\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        \n        loss = outputs.loss\n        if torch.cuda.device_count() > 1:\n            loss = loss.mean()\n        total_train_loss += loss.item()\n        \n        # Backward pass: compute gradients.\n        loss.backward()\n        \n        # Update weights.\n        optimizer.step()\n        \n        # Update learning rate.\n        scheduler.step()\n        \n        progress_bar.set_postfix({'loss': loss.item()})\n\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n\n    # Validation Phase\n    model.eval() # Put the model in evaluation mode.\n    total_eval_loss = 0\n    total_eval_accuracy = 0\n    \n    with torch.no_grad(): # Disable gradient calculation for efficiency.\n        for batch in tqdm(val_dataloader, desc=\"Validation\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            \n            loss = outputs.loss\n            if torch.cuda.device_count() > 1:\n                loss = loss.mean()\n            total_eval_loss += loss.item()\n            \n            # Calculate accuracy\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)\n            total_eval_accuracy += (predictions == labels).sum().item()\n\n    avg_val_loss = total_eval_loss / len(val_dataloader)\n    avg_val_accuracy = total_eval_accuracy / len(val_dataset)\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n    print(f\"Validation Accuracy: {avg_val_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T12:26:40.465327Z","iopub.execute_input":"2025-08-30T12:26:40.466166Z","iopub.status.idle":"2025-08-30T14:16:08.360868Z","shell.execute_reply.started":"2025-08-30T12:26:40.466135Z","shell.execute_reply":"2025-08-30T14:16:08.359759Z"}},"outputs":[{"name":"stdout","text":"\n--- Epoch 1/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1240 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bc53258c8ac46f3b401a55a6d66b4b9"}},"metadata":{}},{"name":"stdout","text":"Average Training Loss: 0.1769\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/310 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe954477152b46259773c84747dfc86f"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.1438\nValidation Accuracy: 0.9482\n\n--- Epoch 2/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1240 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78e2b72fdf454d4099de90fe7905d225"}},"metadata":{}},{"name":"stdout","text":"Average Training Loss: 0.0960\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/310 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"470156067c264b639a48b44157fb3098"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.1451\nValidation Accuracy: 0.9509\n\n--- Epoch 3/3 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1240 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08958244ff747fdb0d4b2f1a74f9e89"}},"metadata":{}},{"name":"stdout","text":"Average Training Loss: 0.0530\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/310 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b7a6e45cefb4090b3a50ec559e74fa4"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.1677\nValidation Accuracy: 0.9509\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"model.module.save_pretrained('/kaggle/working/roberta-sentiment-analysis')\ntokenizer.save_pretrained('/kaggle/working/roberta-sentiment-tokenizer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T14:24:29.445191Z","iopub.execute_input":"2025-08-30T14:24:29.446156Z","iopub.status.idle":"2025-08-30T14:24:30.849151Z","shell.execute_reply.started":"2025-08-30T14:24:29.446106Z","shell.execute_reply":"2025-08-30T14:24:30.848267Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/roberta-sentiment-tokenizer/tokenizer_config.json',\n '/kaggle/working/roberta-sentiment-tokenizer/special_tokens_map.json',\n '/kaggle/working/roberta-sentiment-tokenizer/vocab.json',\n '/kaggle/working/roberta-sentiment-tokenizer/merges.txt',\n '/kaggle/working/roberta-sentiment-tokenizer/added_tokens.json')"},"metadata":{}}],"execution_count":18}]}